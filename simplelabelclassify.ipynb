{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "losses: 5.080528 accuracy: 0.1656\n",
      "5\n",
      "losses: 1.3990083 accuracy: 0.6506\n",
      "10\n",
      "losses: 0.83316475 accuracy: 0.766\n",
      "15\n",
      "losses: 0.6164927 accuracy: 0.814\n",
      "20\n",
      "losses: 0.45609835 accuracy: 0.8618\n",
      "25\n",
      "losses: 0.39389247 accuracy: 0.8808\n",
      "30\n",
      "losses: 0.3220664 accuracy: 0.9006\n",
      "35\n",
      "losses: 0.28851238 accuracy: 0.9158\n",
      "40\n",
      "losses: 0.26678145 accuracy: 0.9226\n",
      "45\n",
      "losses: 0.24778128 accuracy: 0.9252\n",
      "50\n",
      "losses: 0.23514155 accuracy: 0.9262\n",
      "55\n",
      "losses: 0.24281958 accuracy: 0.9288\n",
      "60\n",
      "losses: 0.2364804 accuracy: 0.9314\n",
      "65\n",
      "losses: 0.23303199 accuracy: 0.9344\n",
      "70\n",
      "losses: 0.24749583 accuracy: 0.93\n",
      "75\n",
      "losses: 0.23692307 accuracy: 0.934\n",
      "80\n",
      "losses: 0.25376824 accuracy: 0.9318\n",
      "85\n",
      "losses: 0.25586948 accuracy: 0.9332\n",
      "90\n",
      "losses: 0.23818973 accuracy: 0.9334\n",
      "95\n",
      "losses: 0.26053026 accuracy: 0.9314\n",
      "100\n",
      "losses: 0.24245317 accuracy: 0.9364\n",
      "105\n",
      "losses: 0.24811655 accuracy: 0.9418\n",
      "110\n",
      "losses: 0.2506592 accuracy: 0.9376\n",
      "115\n",
      "losses: 0.26194772 accuracy: 0.9376\n",
      "120\n",
      "losses: 0.25457016 accuracy: 0.9368\n",
      "125\n",
      "losses: 0.26645458 accuracy: 0.9346\n",
      "130\n",
      "losses: 0.25729364 accuracy: 0.9372\n",
      "135\n",
      "losses: 0.27116615 accuracy: 0.9372\n",
      "140\n",
      "losses: 0.24571936 accuracy: 0.9404\n",
      "145\n",
      "losses: 0.286548 accuracy: 0.936\n",
      "150\n",
      "losses: 0.27800202 accuracy: 0.9374\n",
      "155\n",
      "losses: 0.2841744 accuracy: 0.9368\n",
      "160\n",
      "losses: 0.26087943 accuracy: 0.9384\n",
      "165\n",
      "losses: 0.25895637 accuracy: 0.9416\n",
      "170\n",
      "losses: 0.27237234 accuracy: 0.9388\n",
      "175\n",
      "losses: 0.26293504 accuracy: 0.9402\n",
      "180\n",
      "losses: 0.30118653 accuracy: 0.9354\n",
      "185\n",
      "losses: 0.30390933 accuracy: 0.9368\n",
      "190\n",
      "losses: 0.2928421 accuracy: 0.938\n",
      "195\n",
      "losses: 0.28438157 accuracy: 0.9386\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import keras as kr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "train_dir = 'cnews/cnews.train.txt'\n",
    "val_dir = 'cnews/cnews.val.txt'\n",
    "test_dir = 'cnews/cnews.test.txt'\n",
    "vocab_dir = 'cnews/cnews.vocab.txt'\n",
    "\n",
    "def read_file(filename):\n",
    "    #读取文本数据\n",
    "    contents = []\n",
    "    labels = []\n",
    "    with open(filename,mode = 'r',encoding = 'utf-8',errors = 'ignore') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                label,content = line.strip().split('\\t')\n",
    "                if content:\n",
    "                    contents.append(list(content))\n",
    "                    labels.append(label)\n",
    "            except:\n",
    "                pass\n",
    "    #print(contents)\n",
    "    return contents,labels\n",
    "\n",
    "def build_vocab(vocab_size):\n",
    "    #构建词汇表\n",
    "    data_train,data_trainlabel = read_file(train_dir)\n",
    "    data_val,data_vallabel = read_file(val_dir)\n",
    "    data_test,data_testlabel = read_file(test_dir)\n",
    "    \n",
    "    all_data = []\n",
    "    for content in data_train:\n",
    "        all_data.extend(content)\n",
    "        \n",
    "    for content in data_val:\n",
    "        all_data.extend(content)\n",
    "        \n",
    "    for content in data_test:\n",
    "        all_data.extend(content)\n",
    "    \n",
    "    counter = Counter(all_data)\n",
    "    count_pairs = counter.most_common(vocab_size-1)\n",
    "    words,_ = list(zip(*count_pairs))\n",
    "    words =['<PAD>'] + list(words)\n",
    "    open(vocab_dir,mode = 'w',encoding = 'utf-8',errors = 'ignore').write('\\n'.join(words)+'\\n')\n",
    "    \n",
    "#build_vocab()\n",
    "def read_vocab():\n",
    "    with open(vocab_dir,mode = 'r',encoding = 'utf-8', errors = 'ignore') as f:\n",
    "        words = [x.strip() for x in f.readlines()]\n",
    "        word_to_id = dict(zip(words,range(len(words))))\n",
    "    return words,word_to_id\n",
    "\n",
    "def read_category():\n",
    "    categories = ['体育','财经','房产','家居','教育','科技','时尚','时政','游戏','娱乐']\n",
    "    cat_to_id = dict(zip(categories,range(len(categories))))\n",
    "    return categories,cat_to_id\n",
    "\n",
    "def process_file(filename,word_to_id,cat_to_id,max_length):\n",
    "    #将文件转成id表示\n",
    "    contents,labels = read_file(filename)\n",
    "    \n",
    "    data_id,label_id = [],[]\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n",
    "        label_id.append(cat_to_id[labels[i]])\n",
    "    \n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id,max_length)\n",
    "    y_pad = kr.utils.to_categorical(label_id,num_classes = len(cat_to_id))\n",
    "    \n",
    "    return x_pad,y_pad\n",
    "\n",
    "def batch_iter(x,y,batch_size):\n",
    "    #生成批次数据\n",
    "    data_len = len(x)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x[indices]\n",
    "    y_shuffle = y[indices]\n",
    "    \n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i+1) * batch_size,data_len)\n",
    "        yield x_shuffle[start_id:end_id],y_shuffle[start_id:end_id]\n",
    "\n",
    "#获取输入数据\n",
    "if not os.path.exists(vocab_dir):\n",
    "    build_vocab(5000)\n",
    "\n",
    "categories,cat_to_id = read_category()\n",
    "words,word_to_id = read_vocab()\n",
    "        \n",
    "#CNN\n",
    "embedding_dim = 64\n",
    "seq_length = 600\n",
    "num_classes = 10\n",
    "vocab_size = len(words)\n",
    "kernel_size = 5\n",
    "num_filters = 128\n",
    "filter_sizes = [3,4,5]\n",
    "drop_keep_prob = 0.5\n",
    "num_epochs = 200\n",
    "\n",
    "x_train,y_train = process_file(train_dir,word_to_id,cat_to_id,seq_length)\n",
    "x_val,y_val = process_file(val_dir,word_to_id,cat_to_id,seq_length)\n",
    "\n",
    "#输入输出\n",
    "input_x = tf.placeholder(shape = [None,seq_length],dtype = tf.int32,name = 'input_x')\n",
    "input_y = tf.placeholder(shape = [None,num_classes],dtype = tf.float32,name = 'input_y')\n",
    "keep_prob = tf.placeholder(tf.float32,name = 'keep_prob')\n",
    "\n",
    "embedding = tf.Variable(tf.random_uniform([vocab_size,embedding_dim],-1.0,1.0),name='embedding')\n",
    "embedding_inputs_chars = tf.nn.embedding_lookup(embedding,input_x)\n",
    "embedding_inputs = tf.expand_dims(embedding_inputs_chars,-1)\n",
    "    \n",
    "pooled_outputs = []\n",
    "for i,filter_size in enumerate(filter_sizes):\n",
    "    with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "        filter_shape = [filter_size,embedding_dim,1,num_filters]\n",
    "        W = tf.Variable(tf.truncated_normal(filter_shape,stddev=0.1),name='W')\n",
    "        b = tf.Variable(tf.constant(0.1,shape=[num_filters]),name='b')\n",
    "        conv = tf.nn.conv2d(embedding_inputs,W,strides=[1,1,1,1],padding='VALID',name='conv')\n",
    "        h = tf.nn.relu(tf.nn.bias_add(conv,b),name='relu')\n",
    "        pooled = tf.nn.max_pool(h,ksize=[1,seq_length - filter_size + 1,1,1],strides=[1,1,1,1],padding='VALID',name='pool')\n",
    "        pooled_outputs.append(pooled)\n",
    "        \n",
    "#获取全连接层的输入\n",
    "num_filters_total = num_filters * len(filter_sizes)        \n",
    "h_pool = tf.concat(pooled_outputs,3)\n",
    "h_pool_flat = tf.reshape(h_pool,[-1,num_filters_total])\n",
    "\n",
    "#连接层\n",
    "with tf.name_scope('dropout'):\n",
    "    h_drop = tf.nn.dropout(h_pool_flat,drop_keep_prob)\n",
    "    \n",
    "with tf.name_scope('output'):\n",
    "    W = tf.Variable(tf.truncated_normal([num_filters_total,num_classes],stddev=0.1),name='W')\n",
    "    b = tf.Variable(tf.constant(0.1,shape=[num_classes]),name='b')\n",
    "    scores = tf.nn.xw_plus_b(h_drop,W,b,name='scores')\n",
    "    predictions = tf.argmax(scores,1,name='predictions')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = scores,labels = input_y))\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_predictions = tf.equal(predictions,tf.argmax(input_y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions,tf.float32),name='accuracy')\n",
    "\n",
    "with tf.name_scope('optimize'):\n",
    "    train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "#gpu配置信息\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.7)\n",
    "config = tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=True,log_device_placement=True)    \n",
    "\n",
    "with tf.Session(config = config) as sess:                     \n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_train = batch_iter(x_train,y_train,embedding_dim)\n",
    "        for x_batch,y_batch in batch_train:\n",
    "            sess.run(train_op,feed_dict = {input_x:x_batch,input_y:y_batch,keep_prob:drop_keep_prob})\n",
    "            #losses,acc = sess.run([loss,accuracy],feed_dict = {input_x:x_batch,input_y:y_batch,keep_prob:1.0})\n",
    "        if epoch % 5 == 0:\n",
    "            print (epoch)\n",
    "            losses,acc = sess.run([loss,accuracy],feed_dict = {input_x:x_val,input_y:y_val,keep_prob:1.0})\n",
    "            print('losses: ' + str(losses) + ' accuracy: ' + str(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
